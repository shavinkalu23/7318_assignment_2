{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1R6kbyiVRJGhzfie2oVQ5JOp46hk3KV_X",
      "authorship_tag": "ABX9TyNhlnpwivelebWzEkPnAOan",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shavinkalu23/7318_assignment_2/blob/main/7318_assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "dXp-EWAp6DUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "494cbf70-e478-4601-faba-b9b5420e0872"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from torchvision.transforms.v2 import ToTensor,Resize,Compose,ColorJitter,RandomRotation,RandomAffine, AugMix,RandomCrop,GaussianBlur,RandomEqualize,RandomHorizontalFlip,RandomVerticalFlip, Normalize\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# If you are using CUDA\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# transform = Compose([\n",
        "#     Resize([32,32]),\n",
        "#     ToTensor(),\n",
        "# ])\n",
        "# Load the entire GTSRB dataset\n",
        "full_dataset = torchvision.datasets.GTSRB(root='./data',\n",
        "                                          split='train',\n",
        "                                          download=True)\n",
        "\n",
        "# 80-20 split\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size\n",
        "\n",
        "train_dataset, test_dataset_org = random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "# Further split training data into training and validation datasets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "\n",
        "train_dataset_org, val_dataset_org = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# def compute_mean_std(loader):\n",
        "#     mean = 0.\n",
        "#     std = 0.\n",
        "#     nb_samples = 0.\n",
        "\n",
        "#     for data, _ in loader:\n",
        "#         batch_samples = data.size(0)\n",
        "#         data = data.view(batch_samples, data.size(1), -1)\n",
        "#         mean += data.mean(2).sum(0)\n",
        "#         std += data.std(2).sum(0)\n",
        "#         nb_samples += batch_samples\n",
        "\n",
        "#     mean /= nb_samples\n",
        "#     std /= nb_samples\n",
        "\n",
        "#     return mean, std\n",
        "\n",
        "# train_loader_for_stats = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "# mean, std = compute_mean_std(train_loader_for_stats)\n",
        "# print(mean,std)\n",
        "\n",
        "mean = (0.3337, 0.3064, 0.3171)\n",
        "std =  ( 0.2672, 0.2564, 0.2629)\n",
        "\n",
        "train_transforms = Compose([\n",
        "    Resize([32,32]),\n",
        "    ToTensor(),\n",
        "    Normalize((mean), (std))\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "validation_transforms = Compose([\n",
        "    Resize([32,32]),\n",
        "    ToTensor(),\n",
        "    Normalize((mean), (std))\n",
        "])\n",
        "\n",
        "test_transforms = Compose([\n",
        "    Resize([32,32]),\n",
        "    ToTensor(),\n",
        "    Normalize((mean), (std))\n",
        "])\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.subset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "\n",
        "# Apply transformations\n",
        "train_dataset = CustomDataset(train_dataset_org, transform=train_transforms)\n",
        "val_dataset = CustomDataset(val_dataset_org, transform=validation_transforms)\n",
        "test_dataset = CustomDataset(test_dataset_org, transform=test_transforms)\n",
        "\n",
        "BS = 64\n",
        "\n",
        "# Data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BS, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BS, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BS, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def training_metrics(positives, data_size, loss):\n",
        "    acc = positives / data_size\n",
        "    return loss, acc\n",
        "\n",
        "def validation_metrics(validation_data, loss_function, model):\n",
        "    data_size = len(validation_data.dataset)\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    val_loss = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for step, (input, label) in enumerate(validation_data):\n",
        "            input, label = input.to(device), label.to(device)\n",
        "            prediction = model(input)\n",
        "            loss = loss_function(prediction, label)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(prediction, 1)\n",
        "            correct_predictions += (predicted == label).sum().item()\n",
        "            total_samples += label.size(0)\n",
        "\n",
        "    val_acc = correct_predictions / total_samples\n",
        "    return val_loss / len(validation_data), val_acc\n"
      ],
      "metadata": {
        "id": "NpeRK0OU0VC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compile(train_data, validation_data, epochs, loss_function, optimizer, model, learning_rate_scheduler= None, early_stopping = None):\n",
        "    val_acc_list = []\n",
        "    val_loss_list = []\n",
        "\n",
        "    train_acc_list = []\n",
        "    train_loss_list = []\n",
        "\n",
        "    learning_rate_list = []\n",
        "\n",
        "    print('Training started ...')\n",
        "    STEPS = len(train_data)\n",
        "    for epoch in range(epochs):\n",
        "        lr = optimizer.param_groups[0][\"lr\"]\n",
        "        learning_rate_list.append(lr)\n",
        "        correct_predictions = 0\n",
        "        total_examples = 0\n",
        "        loss_val = 0\n",
        "\n",
        "        pbar = tqdm(enumerate(train_data), total=STEPS, desc=f\"Epoch [{epoch+1}/{epochs}]\")\n",
        "        for step, (input, label) in pbar:\n",
        "            input, label = input.to(device), label.to(device)\n",
        "            prediction = model(input)\n",
        "\n",
        "            _, predicted = torch.max(prediction, 1)\n",
        "            correct_predictions += (predicted == label).sum().item()\n",
        "            total_examples += label.size(0)\n",
        "\n",
        "            loss = loss_function(prediction, label)\n",
        "            loss_val += loss.item()\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                \"Learning Rate\": lr,\n",
        "                \"Loss\": f\"{loss.item():.4f}\",\n",
        "                \"Accuracy\": f\"{correct_predictions/total_examples:.4f}\"\n",
        "            })\n",
        "\n",
        "        training_loss, training_acc = training_metrics(correct_predictions, total_examples, loss_val / len(train_data))\n",
        "        train_acc_list.append(training_acc)\n",
        "        train_loss_list.append(training_loss)\n",
        "\n",
        "        val_loss, val_acc = validation_metrics(validation_data, loss_function, model)\n",
        "        val_acc_list.append(val_acc)\n",
        "        val_loss_list.append(val_loss)\n",
        "\n",
        "        print(f'Validation Accuracy: {val_acc:.4f}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "        if early_stopping:\n",
        "          early_stopping.step(val_loss)\n",
        "          if early_stopping.stop:\n",
        "              break\n",
        "        if learning_rate_scheduler:\n",
        "          learning_rate_scheduler.step()\n",
        "\n",
        "    metrics_dict = {\n",
        "        'train_acc': train_acc_list,\n",
        "        'train_loss': train_loss_list,\n",
        "        'val_acc': val_acc_list,\n",
        "        'val_loss': val_loss_list,\n",
        "        'learning_rate': learning_rate_list\n",
        "    }\n",
        "\n",
        "    return metrics_dict"
      ],
      "metadata": {
        "id": "mQ27GYQ33WCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
        "        self.fc1 = nn.Linear(32 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 43)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "\n",
        "model = BaseCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "base_cnn_metrics = compile(model =model, train_data=train_loader,validation_data=val_loader,epochs=EPOCHS,loss_function=loss,optimizer=optimizer)"
      ],
      "metadata": {
        "id": "w11b_UPIFT8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28009e0d-c398-4fdf-bcc1-0dcd85aee1d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [1/5]: 100%|██████████| 267/267 [00:16<00:00, 16.34it/s, Learning Rate=0.001, Loss=3.6109, Accuracy=0.0503]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.0713, Validation Loss: 3.5707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [2/5]: 100%|██████████| 267/267 [00:07<00:00, 34.99it/s, Learning Rate=0.001, Loss=3.4021, Accuracy=0.0980]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.1239, Validation Loss: 3.3560\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [3/5]: 100%|██████████| 267/267 [00:09<00:00, 27.65it/s, Learning Rate=0.001, Loss=3.5403, Accuracy=0.1294]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.1558, Validation Loss: 3.1991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [4/5]: 100%|██████████| 267/267 [00:11<00:00, 22.71it/s, Learning Rate=0.001, Loss=2.4748, Accuracy=0.2157]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.2740, Validation Loss: 2.7057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [5/5]: 100%|██████████| 267/267 [00:11<00:00, 23.22it/s, Learning Rate=0.001, Loss=2.0912, Accuracy=0.3470]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4680, Validation Loss: 1.9527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, '/content/drive/My Drive/base_model.pth')"
      ],
      "metadata": {
        "id": "wH606x9KFUOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation can introduce variance into the dataset and reduce overfitting. Given the GTSRB dataset contains traffic signs which can be captured under various conditions, we can use a few common augmentations:"
      ],
      "metadata": {
        "id": "iHNt5DRS0CYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = (0.3337, 0.3064, 0.3171)\n",
        "std =  ( 0.2672, 0.2564, 0.2629)\n",
        "\n",
        "train_aug_transforms = Compose([\n",
        "    RandomHorizontalFlip(),\n",
        "    RandomRotation(10),\n",
        "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    Resize([32,32]),\n",
        "    ToTensor(),\n",
        "    Normalize((mean), (std))\n",
        "])\n",
        "train_aug_dataset = CustomDataset(train_dataset , transform=train_aug_transforms)\n",
        "\n",
        "BS = 64\n",
        "\n",
        "# Data loaders\n",
        "train_loader = torch.utils.data.DataLoader(torch.utils.data.ConcatDataset([train_dataset,train_aug_dataset]), batch_size=BS, shuffle=True, num_workers=2, pin_memory=True)\n",
        "model = BaseCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "base_cnn_aug_metrics = compile(model =model, train_data=train_loader,validation_data=val_loader,epochs=EPOCHS,loss_function=loss,optimizer=optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Phx1Y_NJf3Xg",
        "outputId": "07c592ea-26ed-4605-a5de-4fd97146c69b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [1/5]: 100%|██████████| 533/533 [00:59<00:00,  8.98it/s, Learning Rate=0.001, Loss=3.2465, Accuracy=0.0596]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.1006, Validation Loss: 3.4184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [2/5]: 100%|██████████| 533/533 [00:50<00:00, 10.55it/s, Learning Rate=0.001, Loss=3.1005, Accuracy=0.1475]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.2519, Validation Loss: 2.8873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [3/5]: 100%|██████████| 533/533 [00:51<00:00, 10.40it/s, Learning Rate=0.001, Loss=2.4322, Accuracy=0.2748]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4419, Validation Loss: 2.1388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [4/5]: 100%|██████████| 533/533 [00:52<00:00, 10.25it/s, Learning Rate=0.001, Loss=2.2555, Accuracy=0.3919]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.5510, Validation Loss: 1.6191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [5/5]: 100%|██████████| 533/533 [00:52<00:00, 10.06it/s, Learning Rate=0.001, Loss=1.8907, Accuracy=0.4855]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6798, Validation Loss: 1.2066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, '/content/drive/My Drive/base_model_aug.pth')"
      ],
      "metadata": {
        "id": "eEhPRivxg9Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert the images to size 224x224 to support input size of VGG16 and Resnet34\n",
        "train_transforms = Compose([\n",
        "    Resize([224,224]),\n",
        "    ToTensor(),\n",
        "    Normalize((mean), (std))\n",
        "])\n",
        "\n",
        "validation_transforms = Compose([\n",
        "    Resize([224,224]),\n",
        "    ToTensor(),\n",
        "    Normalize((mean), (std))\n",
        "])\n",
        "\n",
        "test_transforms = Compose([\n",
        "    Resize([224,224]),\n",
        "    ToTensor(),\n",
        "    Normalize((mean), (std))\n",
        "])\n",
        "\n",
        "train_aug_transforms = Compose([\n",
        "    RandomHorizontalFlip(),\n",
        "    RandomRotation(10),\n",
        "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    Resize([224,224]),\n",
        "    ToTensor(),\n",
        "    Normalize((mean), (std))\n",
        "])"
      ],
      "metadata": {
        "id": "Pf0vp8dwr9qd"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply transformations\n",
        "train_dataset = CustomDataset(train_dataset, transform=train_transforms)\n",
        "val_dataset = CustomDataset(val_dataset, transform=validation_transforms)\n",
        "test_dataset = CustomDataset(test_dataset, transform=test_transforms)\n",
        "\n",
        "BS = 64\n",
        "\n",
        "train_aug_dataset = CustomDataset(train_dataset , transform=train_aug_transforms)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = torch.utils.data.DataLoader(torch.utils.data.ConcatDataset([train_dataset,train_aug_dataset]), batch_size=BS, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BS, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BS, shuffle=False, num_workers=2, pin_memory=True)\n"
      ],
      "metadata": {
        "id": "qx28R3zYrym7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "LEARNING_RATE = 0.001\n",
        "INPUT_DIM = 3*32*32\n",
        "OUTPUT_DIM = 43\n",
        "# Load the VGG16 model\n",
        "vgg16 = models.vgg16(pretrained=False)  # We use a non-pretrained model for this task\n",
        "vgg16.classifier[6] = nn.Linear(4096, 43)  # Change the last layer to have 43 outputs\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = vgg16.to(device)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "vgg16_1_metrics = compile(model =model, train_data=train_loader,validation_data=val_loader,epochs=EPOCHS,loss_function=loss,optimizer=optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzHkPUo86elX",
        "outputId": "fb9d07f0-1e1b-4a29-cca4-8ecdb57ab2a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [1/5]: 100%|██████████| 533/533 [11:30<00:00,  1.30s/it, Learning Rate=0.001, Loss=2.5757, Accuracy=0.1504]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4063, Validation Loss: 1.9491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [2/5]: 100%|██████████| 533/533 [11:39<00:00,  1.31s/it, Learning Rate=0.001, Loss=1.6182, Accuracy=0.4177]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7673, Validation Loss: 0.7236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [3/5]: 100%|██████████| 533/533 [11:38<00:00,  1.31s/it, Learning Rate=0.001, Loss=1.2378, Accuracy=0.6206]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8968, Validation Loss: 0.3289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [4/5]: 100%|██████████| 533/533 [11:53<00:00,  1.34s/it, Learning Rate=0.001, Loss=0.6690, Accuracy=0.7161]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9510, Validation Loss: 0.1490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [5/5]: 100%|██████████| 533/533 [11:54<00:00,  1.34s/it, Learning Rate=0.001, Loss=0.7558, Accuracy=0.7606]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9704, Validation Loss: 0.0973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, '/content/drive/MyDrive/Colab Notebooks/Models/vgg16.pth')"
      ],
      "metadata": {
        "id": "_N-_uwJdeMOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Load the resnet34 model\n",
        "resnet34 = models.resnet34(pretrained=False)  # We use a non-pretrained model for this task\n",
        "resnet34.fc = nn.Linear(resnet34.fc.in_features, 43)  # Change the final fully connected layer to have 43 outputs\n",
        "\n",
        "model = resnet34.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "resnet_metrics = compile(model =model, train_data=train_loader,validation_data=val_loader,epochs=EPOCHS,loss_function=loss,optimizer=optimizer)\n",
        "torch.save(model, '/content/drive/MyDrive/Colab Notebooks/Models/resnet34.pth')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyX_wXNv7JW9",
        "outputId": "21b193bc-439b-4938-9c6d-b9c8e0e95320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [1/5]: 100%|██████████| 533/533 [04:22<00:00,  2.03it/s, Learning Rate=0.001, Loss=2.9008, Accuracy=0.1354]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.2548, Validation Loss: 3.0069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [2/5]: 100%|██████████| 533/533 [04:14<00:00,  2.10it/s, Learning Rate=0.001, Loss=2.2076, Accuracy=0.2460]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4764, Validation Loss: 1.6113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [3/5]: 100%|██████████| 533/533 [04:14<00:00,  2.10it/s, Learning Rate=0.001, Loss=1.5765, Accuracy=0.4045]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7011, Validation Loss: 0.9228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [4/5]: 100%|██████████| 533/533 [04:17<00:00,  2.07it/s, Learning Rate=0.001, Loss=1.4903, Accuracy=0.5629]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8689, Validation Loss: 0.4273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [5/5]: 100%|██████████| 533/533 [04:13<00:00,  2.10it/s, Learning Rate=0.001, Loss=1.0591, Accuracy=0.6275]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8820, Validation Loss: 0.3333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SxydjxCdzEIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.stop = False\n",
        "\n",
        "    def step(self, val_loss):\n",
        "        score = -val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "        elif score < self.best_score + self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "99Y7jBo8XDi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample randomly from the hyperparameter space\n",
        "hyperparameters_space = {\n",
        "    'batch_size': [16, 32, 64, 128],\n",
        "    'lr': [1e-3, 1e-4, 5e-5, 5e-4],\n",
        "    'momentum': [0.9, 0.95, 0.99],\n",
        "    'optimizer': ['Adam','SGD'],\n",
        "    'weight_decay': [0, 1e-5, 1e-4],   # Regularization for Adam\n",
        "}\n",
        "\n",
        "num_samples = 5\n",
        "random_search = []\n",
        "import random\n",
        "for _ in range(num_samples):\n",
        "    sample = {}\n",
        "    for k, v in hyperparameters_space.items():\n",
        "        sample[k] = random.choice(v)\n",
        "    random_search.append(sample)\n",
        "\n",
        "random_search = random.sample(random_search, num_samples)\n",
        "print(random_search)\n",
        "\n",
        "\n",
        "best_accuracy = 0.0\n",
        "best_params = None\n",
        "MAX_EPOCHS = 5  # Setting an upper bound on epochs, you can adjust this\n",
        "results = []\n",
        "for params in random_search:\n",
        "    # Define data loaders with the current batch size\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BS, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BS, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # Define model, loss, optimizer with current hyperparameters\n",
        "    model = models.vgg16(pretrained=False)\n",
        "    model.classifier[6] = nn.Linear(4096, 43)\n",
        "    model.to(device)\n",
        "    # Define optimizer based on the current hyperparameters\n",
        "    if params['optimizer'] == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=params['lr'], momentum=params['momentum'])\n",
        "    elif params['optimizer'] == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
        "    ...\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Initialize early stopping\n",
        "    early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
        "\n",
        "    # Use the compile function for training\n",
        "    metrics = compile(train_loader, val_loader, MAX_EPOCHS, criterion, optimizer, model)\n",
        "\n",
        "\n",
        "    # Assuming the last value in the val_acc_list is for the latest epoch\n",
        "    accuracy = metrics['val_acc'][-1]\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_params = params\n",
        "\n",
        "\n",
        "    results.append({\n",
        "        'batch_size': params['batch_size'],\n",
        "        'lr': params['lr'],\n",
        "        'momentum': params['momentum'],\n",
        "        'optimizer': params['optimizer'],\n",
        "        'weight_decay': params['weight_decay'],\n",
        "        'val_acc': val_acc\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(results, index=False)\n",
        "df.to_csv('hyper_results')\n",
        "print(f\"Best validation accuracy: {best_accuracy}\")\n",
        "print(f\"Best hyperparameters: {best_params}\")"
      ],
      "metadata": {
        "id": "HGCtU6uuXOfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Best Model\n",
        "BS = 32\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "MOMENTUM = 0.95\n",
        "# Apply transformations\n",
        "train_dataset = CustomDataset(train_dataset_org, transform=train_transforms)\n",
        "val_dataset = CustomDataset(val_dataset_org, transform=validation_transforms)\n",
        "test_dataset = CustomDataset(test_dataset_org, transform=test_transforms)\n",
        "\n",
        "train_aug_dataset = CustomDataset(train_dataset , transform=train_aug_transforms)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = torch.utils.data.DataLoader(torch.utils.data.ConcatDataset([train_dataset,train_aug_dataset]), batch_size=BS, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BS, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BS, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# Load the VGG16 model\n",
        "vgg16 = models.vgg16(pretrained=False)  # We use a non-pretrained model for this task\n",
        "vgg16.classifier[6] = nn.Linear(4096, 43)  # Change the last layer to have 43 outputs\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = vgg16.to(device)\n",
        "optimizer = optim.SGD(params=model.parameters(),lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "lr_s = optim.lr_scheduler.LinearLR(optimizer,start_factor=1.0,end_factor=0.5,total_iters=10)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n"
      ],
      "metadata": {
        "id": "_fQbs-xb39fh"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_final_metrics = compile(model =model, train_data=train_loader,validation_data=val_loader,epochs=EPOCHS,loss_function=loss,optimizer=optimizer,learning_rate_scheduler=lr_s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "42UxM_lx8lw3",
        "outputId": "951a720e-76ac-41ea-d82d-c96dd8d057b4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch [1/10]:   0%|          | 0/1066 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "Epoch [1/10]: 100%|██████████| 1066/1066 [08:22<00:00,  2.12it/s, Learning Rate=0.001, Loss=2.3459, Accuracy=0.1557]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.3924, Validation Loss: 1.9197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch [2/10]:   0%|          | 0/1066 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "Epoch [2/10]:  37%|███▋      | 394/1066 [03:06<05:17,  2.11it/s, Learning Rate=0.00095, Loss=2.1974, Accuracy=0.3706]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-67fdd9d541f1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvgg_final_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-3c252474d474>\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(train_data, validation_data, epochs, loss_function, optimizer, model, learning_rate_scheduler, early_stopping)\u001b[0m\n\u001b[1;32m     35\u001b[0m             pbar.set_postfix({\n\u001b[1;32m     36\u001b[0m                 \u001b[0;34m\"Learning Rate\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{loss.item():.4f}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0;34m\"Accuracy\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{correct_predictions/total_examples:.4f}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             })\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(metrics_dict):\n",
        "    epochs = range(1, len(metrics_dict['train_acc']) + 1)\n",
        "\n",
        "    # Plot training and validation accuracy\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, metrics_dict['train_acc'], label='Training Accuracy', marker='o')\n",
        "    plt.plot(epochs, metrics_dict['val_acc'], label='Validation Accuracy', marker='o')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, metrics_dict['train_loss'], label='Training Loss', marker='o')\n",
        "    plt.plot(epochs, metrics_dict['val_loss'], label='Validation Loss', marker='o')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "JzyO4Gtz8v8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(vgg_2_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "EIWzcNjBO3qi",
        "outputId": "cb2f3a47-81b1-475e-f644-ef999060a839"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-38ae8066c60c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg_2_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'plot_metrics' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, '/content/drive/MyDrive/Colab Notebooks/Models/vgg_16_final.pth')"
      ],
      "metadata": {
        "id": "0TPKqzFnO7ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(data_loader, model, loss_function):\n",
        "    data_size = len(data_loader.dataset)\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for step, (input, label) in enumerate(data_loader):\n",
        "            input, label = input.to(device), label.to(device)\n",
        "            prediction = model(input)\n",
        "            loss = loss_function(prediction, label)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(prediction, 1)\n",
        "            correct_predictions += (predicted == label).sum().item()\n",
        "            total_samples += label.size(0)\n",
        "\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    average_loss = total_loss / len(data_loader)\n",
        "    return average_loss, accuracy\n",
        "\n",
        "# Now, to compute the accuracy on the test dataset:\n",
        "\n",
        "test_loss, test_accuracy = evaluate(test_loader, model, loss_function = loss)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}, Test Loss: {test_loss:.4f}\")"
      ],
      "metadata": {
        "id": "imERcjBiUJMo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}